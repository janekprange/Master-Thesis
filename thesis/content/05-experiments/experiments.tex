\chapter{Experiments}
\label{sec:experiments}

% TODO: introduction

\section{Environment and Hardware}
\label{sec:experiments:environmentHardware}
The experiments were conducted on a SLURM cluster using nodes with \num{32} CPU cores, \SI{128}{\giga\byte} RAM and an Nvidia A-100 GPU with \SI{40}{\giga\byte} VRAM. The implementation is written in python 3.10.13. The data is stored in a SQLite database, the database schema can be found in Appendix~\ref{sec:appendix:databaseSchema}.


\input{content/05-experiments/_experimental_setup.tex}


%! debugging
% \section{Evaluating the Synthetic Dataset}
% \label{sec:experiments:syntheticDataset}
% \begin{itemize}
%   \item repetitions in answer from the \ac{llm} during the attribute sentence generation
%         \begin{itemize}
%           \item sometimes models do not produce sensible answers and instead repeat the same phrase multiple times
%           \item this is an unwanted behavior which would drastically worsen the quality of the synthetic dataset
%           \item count the number of repeating 10-grams in all descriptions and attribute sentences % better wording; answers to the prompts? text generations?
%         \end{itemize}
%   \item ensure that the attribute vector dimension are used roughly equally over the different groups
%         \begin{itemize}
%           \item if most dimensions would be used to describe only one group, the distinction between the other groups would get a lot more difficult
%         \end{itemize}
%   \item ensure that for every answer some dimension of the attribute vector is used; this is important for the generation of the \ac{sfam} training dataset (Section~\ref{sec:experiments:setup:sfam})
%         % \item how often attribute vector dimensions have been used -> targeted dimension very often, for the others it would be more interesting
% \end{itemize}

\section{Model performance}
\label{sec:experiments:models}

\begin{itemize}
  \item \ac{sfam} is evaluated on a test dataset that is created the same way as the training dataset (see Section~\ref{sec:experiments:setup:sfam})
        \begin{itemize}
          \item task is to predict if a attribute sentence fits a text
          \item ground trouth is if the sentence was used to describe the answer or not in the attribute sentence generation (see Section~\ref{sec:experiments:setup:sentenceGeneration})
          \item the prediction of \ac{sfam} is a value between \num{0} and \num{1}; it is rounded and compared to the ground truth
          \item the evaluation metrics are accuracy, F1
        \end{itemize}
  \item \ac{lisa} is evaluated against attribute vectors created by \ac{sfam}
        \begin{itemize}
          \item evaluation metrics are MSE, MAE and the average cosine similarity between the attribute vectors
          \item additionally, both vectors are rounded so they represent the prediction if the attribute fits or not; on that, accuracy and F1 is used
        \end{itemize}
  \item the embedding model is evaluated by comparing the embeddings of three answers, two of which are from the same group
        \begin{itemize}
          \item the task is for the model to produce embeddings that are closer to each other for answers from the same group
          \item evaluation metrics are accuracy and F1 for both cosine and euclidean distance
        \end{itemize}
\end{itemize}

\section{Steering Performance}
\label{sec:experiments:steering}
\begin{itemize}
  \item
\end{itemize}

