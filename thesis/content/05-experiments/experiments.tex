\chapter{Experiments}
\label{sec:experiments}

% TODO: introduction
% \begin{itemize}
%   \item to conduct the the experiments in this section, the approach presented in the previous section first have to be implemented
%   \item this means that the synthetic dataset has to be created, the dimensions of the interpretable attribute vector have to be selected, the models have to be trained an the activation-based steering vectors have to be extracted
% \end{itemize}

\section{Environment and Hardware}
\label{sec:experiments:environmentHardware}
The experiments were conducted on a SLURM cluster using nodes with \num{32} CPU cores, \SI{128}{\giga\byte} RAM and an Nvidia A-100 GPU with \SI{40}{\giga\byte} VRAM. The implementation is written in python 3.10.13. The data was stored in a SQLite database, the database schema can be found in Appendix~\ref{sec:appendix:databaseSchema}.


\input{content/05-experiments/_experimental_setup.tex}


\section{Evaluating the Importance of Knowledge Attributes for the Interpretable Attribute Vector}%
\label{sec:experiments:knowledgeAttributes}
Knowledge attributes are an extension of the state-of-the-art style vector and a novel contribution of this thesis. This experiment is designed to address research question~\ref{rq:interpretableGroupDetect:knowledgeAttributes} by evaluating the significance of the knowledge attributes in the context of group membership detection tasks.

It involves analyzing the median attribute vectors for each group across all answers in the synthetic dataset introduced in Sections~\ref{sec:approach:attributeSentenceGeneration} and \ref{sec:experiments:setup:sentenceGeneration}. For each group, the dimensions of the interpretable attribute vector are ordered by their importance in distinguishing the target group from the others. The importance of each dimension is measured using the point-biserial correlation coefficient, similar to the methodology described in Section~\ref{sec:experiments:setup:steering:prompt}.

If the most important attributes for group differentiation do not include knowledge attributes, then knowledge attributes are not essential for group membership detection. Conversely, if knowledge dimensions are among the top-ranked attributes, this would indicate their value in enhancing group-specific text representations.


\section{Testing Model Performance}
\label{sec:experiments:models}
The three models that are presented in this thesis, \ac{sfam}, \ac{lisa} and the embedding model, are all trained on test datasets that are constructed similarly to their training and validation datasets but consist of unseen data.

For \textbf{\ac{sfam}}, the test dataset is constructed from unseen answers of the synthetic dataset described in Section~\ref{sec:approach:attributeSentenceGeneration}. The test data is balanced and consists of \num{\sfamTestDataSize} samples which were extracted with the method described in Section~\ref{sec:experiments:setup:sfam}.
During the test, \ac{sfam} is tasked with predicting if an attribute sentence matches a given answer. The ground truth for this task is if the attribute sentence was actually used to describe the answer in the synthetic dataset. The agreement score that \ac{sfam} produces is rounded to either one (positive prediction) or zero (negative prediction) and compared to the ground truth with accuracy and F1 as the metric.

\textbf{\ac{lisa}} is trained and evaluated attribute vectors created with \ac{sfam}. The evaluation is conducted on \num{\lisaTestDataSize} unseen texts. The model is evaluated with the metrics \ac{mse}, \ac{mae} and the average cosine similarity between the attribute vectors produced by \ac{sfam} and \ac{lisa}. Additionally, the attribute vectors from both models are rounded to produce a prediction if specific attributes are present in a given text. These predictions are subsequently compared with the accuracy and F1 metric.

The \textbf{embedding model}, which is trained using the same attribute vectors as \ac{lisa}, is evaluated based on its ability to detect group membership. This evaluation consists of two types of experiments. The first experiment tests the model in a manner consistent with its training approach. For each iteration of the evaluation, three embeddings are selected: an anchor embedding, a positive embedding from the same group as the anchor, and a negative embedding from a different group. The evaluation determines whether the anchor embedding is closer to the positive than to the negative one. Although the model is trained using Euclidean distance, the evaluation uses both Euclidean and cosine distances. Results are reported using accuracy and the F1 score.

The second experiment evaluates the model based on its ability to assign embeddings to the correct group using median group embeddings. First, median embeddings are computed for each group using the training and validation data. Then, for each test embedding, the distance to the median of each group is calculated. The model then predicts the group to which the test embedding is closest. The prediction is considered correct if the closest median matches the actual group of the test embedding. Accuracy and F1 score are again used to evaluate this setup, with both Euclidean and cosine distance as the basis.

To evaluate the applicability of the embedding model across different domains, the same two experiments are repeated using the AskX dataset, introduced in Section~\ref{sec:datasets:askx}. Unlike the Stack Exchange dataset described in Section~\ref{sec:datasets:stackex}, the AskX dataset features stylistic and conceptual differences, including distinct writing styles and different represented groups. This domain shift tests the robustness of the model and the relevance of the learned attributes in unfamiliar settings. For the triplet-based evaluation, triplets are constructed from group-specific answers in AskX, and the predictions of the model are evaluated as before. For the median-based test, the AskX dataset is divided such that one half is used to compute the median group embeddings, while the other half serves as the evaluation set. This ensures that no overlap biases the results and that the generalization performance is accurately measured.

These evaluations are particularly important for understanding the practical utility of the embedding model. Since overly specific attributes such as \enquote{The author uses computer science concepts} may perform well within the original domain but poorly in new contexts, testing on the AskX dataset helps to identify which attributes are genuinely generalizable and contribute meaningfully to cross-domain group membership detection.

%? variance inside groups

\section{Testing Steering Performance}
\label{sec:experiments:steering}
To ensure comparable results, all steering methods introduced in previous sections are compared using the same experiments. The evaluation uses questions from the steering dataset (see Section~\ref{sec:datasets:steering}). The synthetic dataset (see Section~\ref{sec:approach:attributeSentenceGeneration}) is used to identify the groups to steer the explanations towards and to extract the most important attributes of the groups.

First, for each question, the \ac{llm} produces an unsteered explanation that acts as a baseline.
Then, for each  question and group, the \ac{llm} generates an explanation while being steered by one of the steering methods presented in Section~\ref{sec:experiments:setup:steering}. After that, the explanations are being embedded using the embedding model introduced in Section~\ref{sec:approach:embedding}. These embeddings are finally compared to the median embedding of all answers in the synthetic dataset from that group to measure if the steering method had an effect compared to the unsteered baseline generation.
To extract the group embedding, the median of all embeddings is used opposed to the mean because it is less sensitive to outliers. This is necessary as there is no guarantee that the group-specific answers that are being used for the creation of the synthetic dataset are actually written by members of that group.

All steering approaches are evaluated with for different metrics that describe the steering effect. As Figure~\ref{fig:steeringMetrics} shows, the first metric is the \textbf{steering direction correctness}, which describes if the method steers the explanation towards the style and background knowledge of the correct group. This is an important metric as a very strong steering effect would not be helpful if it steers towards the wrong group. The second metric is the \textbf{steering effect} itself. It is computed from the Euclidean distance between the embeddings of the unsteered and steered explanation and describe how strong the explanation was changed by the steering methods.

Finally, there are two more metrics that show how strong the steering effect could have been. This is important as a low steering effect is not as bad if the unsteered embedding is already very close to the median group embedding. These metrics are the \textbf{optimal steering effect}, which is the Euclidean distance between the unsteered embedding and the median group embedding, and the \textbf{possible improvement}. It is the Euclidean distance between the embedding of the steered explanation and the median group embedding and describes how much better the steering effect could have been if the direction or strength of steering would be better.

%? the absolute times that the resulting group is the correct one (according to the embedding)

\begin{figure}[ht]
  \begin{center}
    \input{figures/tikz/steering-metrics-plot.tex}
  \end{center}
  \captionsetup{singlelinecheck=off}
  \caption[]{%
    All answers that are generated during steering are embedded using the \ac{lisa} model with the embedding head. For each question, the \ac{llm} is prompted without any steering to create an unsteered baseline. Then, for each group and each steering type, the model is prompted with the same question. The median group embedding is derived from the embeddings of all answers that have been used to train \ac{sfam} and \ac{lisa}.
    The quality of each steering method is measured with different metrics:
    \begin{enumerate*}
      \item \textbf{Steering Effect:} The Euclidean distance of the steering effect vector.
      \item \textbf{Steering Direction Correctness:} The cosine similarity between the steering effect vector and the optimal steering effect vector.
      \item \textbf{Possible Steering Effect:} The Euclidean distance between the unsteered embedding and the median group embedding.
      \item \textbf{Possible Improvement:} The Euclidean distance between the steered embedding and the median group embedding.
            % \item difference between steering effect and optimal steering effect
            % \item difference (or ratio?) between steering effect and possible steering effect
    \end{enumerate*}
  }%
  \label{fig:steeringMetrics}
\end{figure}

\subsection{Activation Steering Performance}
The activation steering is evaluated with the same experiments as the prompt steering. Additionally, both steering methods are combined so that there are four different activation steering methods in total. % TODO: mention names of methods?

However, there are two hyperparameters for the activation steering that have to be evaluated as well. For one, it is the layer or layers that are manipulated with the steering vector. According to the work of \citet{konenStyleVectorsSteering2024,bogdanEmergentEffectsScaling2025}, information with the complexity level of the attribute sentences are held in the layers in the middle of the model. Since the \ac{llm} Llama3.2-3B Instruct that has been used for this task has \num{28} layers in total, for these experiments the layers between \num{13} and \num{17} have been chosen. Additionally, the test is conducted with multiple layers being used for steering in parallel.
The other hyperparameter is the scaling factor \(\lambda\) that the steering vector is multiplied with before steering. If \(\lambda\) is too low, there will be no steering effect, but if it is too high the model will not be able to properly predict the next token. For these experiments, the values \num{0.25} and \num{0.5} have been chosen.
To find the best hyperparameter values, a simple grid search is used. Because of the time constraints of the thesis, only a part of the hyperparameter space can be searched.
