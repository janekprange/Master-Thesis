
\section{Experimental Setup}
\label{sec:experiments:setup}

% TODO: introduction?

\subsection{Attribute Sentence Generation}
\label{sec:experiments:setup:sentenceGeneration}
The synthetic dataset that consists of attribute sentences which describe a set of input texts is constructed by prompting a \iac{llm} in a two-step procedure. For this thesis, the open source model Llama3.2-3B Instruct\footnote{\url{https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct}}~(\cite{dubeyLlama3Herd2024}) with \num{3} billion parameters has been used. This model has state-of-the-art performance and is freely available, which provides the possibility to run the model directly on the server cluster. Additionally, it improves the reproducibility of the experiments because it is easier to use the exact same model later.
To improve the performance of the sentence generation, the library vLLM\footnote{\url{https://github.com/vllm-project/vllm}} was used to run the model.

The basis for the generation of the attribute sentences are \num{\numAnswersStyleVector} answers from the stack exchange dataset described in Section %~\ref{}. TODO:
For each of the answers, the model is prompted \num{\numPrompts} times to create descriptions of the text with the method presented in Section~\ref{sec:approach:attributeSentenceGeneration}.

For each of the resulting \num{\numStyleDescriptions} descriptions, the model is prompted again to convert the description into a list of sentences of the form \enquote{The author is \ldots} or \enquote{The author uses \ldots}. The output of the \ac{llm} is split into sentences using the Natural Language Toolkit\footnote{\url{https://www.nltk.org/}} library (\cite{birdNaturalLanguageProcessing2009}). The naive approach of splitting the sentences at the punctuation would not work because they could contain punctuation with expressions like \enquote{e.g.}.

The model is instructed to avoid negations and examples because they can lead to the sentences having too dissimilar a shape while conveying the same meaning. In addition to this constraint that is stated in the prompt, the final sentences are checked to not include the word \enquote{not}. If they do, they are skipped and not used for further computation. During the experiments, \num{\numSentencesWithNegations} sentences have been filtered for this reason.

After the previous steps, \numStyleSentencesNotUniqueText{} attribute sentences have been produced. After taking into account only unique sentences, \numStyleSentencesText{} attribute sentences are left. Of these, \num{\numStyleSentencesStyle} sentences are style sentences, which means they have been produced by a style prompt, and \num{\numStyleSentencesKnowledge} are knowledge prompts.
% TODO: are style sentences + knowledge sentences = all sentences? if not, explain
% TODO: include plot?
% \input{generated/number_of_attributes_per_answer.pgf}

\subsection{Clustering}
\label{sec:experiments:setup:clustering}
Although checking for exact duplicates reduces the number of sentences significantly, there are still many sentences that have basically the same meaning, but are not exactly the same. This can lead to the problem that it would not be clear if an attribute sentence has rarely been used to describe the input texts or if the \ac{llm} has just a high syntactic variance in describing one concept.

The solution to this problem that this thesis proposes is to cluster the sentences and use the clusters as attributes by using the sentence that is closest to the center as the representation for the cluster.
The first step for this procedure is to create an embedding of all sentences so they can be compared semantically. The SBERT model \textit{all-MiniLM-L12-v2}\footnote{\url{https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2}} (\cite{reimersSentenceBERTSentenceEmbeddings2019}) was used to create the embeddings.

For the following steps, the style and knowledge sentences have been processed separately to ensure the resulting clusters are either style or knowledge clusters.
The clustering algorithm first computes a radius neighbor graph of all sentences. All sentences that have a cosine similarity higher than \num{\minCosineSimilarity} are considered to be in the same cluster. Subsequently, the clusters are sorted by size and inspected from largest to smallest. If a sentence is included in one cluster, it is removed from all smaller ones. It is important to note that there is no minimum size for a cluster; if a sentence has no neighbors or every one of them is already part of a larger cluster, then the cluster size is one.

% TODO: why 0.85 min similarity?

At this point, there are \num{\numClusters} clusters, \num{\numClustersStyle} of which are style clusters and \num{\numClustersKnowledge} are knowledge clusters. The size of the complete synthetic dataset can be found in Table~\ref{table:syntheticDataset}.

\begin{table}[ht]
  \caption{The size of the synthetic dataset created for this thesis. The answers are the group-specific input texts that are taken from the Stack Exchange dataset (see Section~\ref{sec:datasets:stackex}). For each answer and prompt, the \ac{llm} is prompted to create the style and knowledge descriptions. The descriptions are converted to attribute sentences by prompting the model again. Finally, the sentences are clustered together by the cosine similarity of their embeddings. The embeddings were produced with an SBERT model (\cite{reimersSentenceBERTSentenceEmbeddings2019}).}
  \begin{center}
    \begin{tabular}{lS[table-format=7.0]}
      \toprule
                   & {Number of data points} \\ \midrule
      Answers      & \numAnswersStyleVector  \\
      Prompts      & \numPrompts             \\
      Descriptions & \numStyleDescriptions   \\
      Sentences    & \numStyleSentences      \\
      Clusters     & \numClusters            \\ \bottomrule
    \end{tabular}%
  \end{center}%
  \label{table:syntheticDataset}
\end{table}

\subsection{Selecting the Attribute Vector Dimensions}
\label{sec:experiments:setup:selection}
The synthetic dataset resulting from the previous steps consists of \num{\numClusters} clusters, which are the potential dimensions of the interpretable attribute vector. This vector however consists of \num{\styleVectorSize} dimensions as established by \citet{patelLearningInterpretableStyle2023}. Therefore, the best clusters have to be selected to be part of the attribute vector. This selection is carried out in multiple steps, which are described in the following section and additionally shown in Figure~\ref{fig:clustering}.

\begin{figure}[h!t]
  \begin{center}
    % \input{figures/tikz/clustering-diagram.tex}
    \includegraphics[width=9cm]{figures/clustering_diagram.png}
  \end{center}
  \caption{The approach so far (see Figure~\ref{fig:attributeSentenceGeneration}) has produced about \numStyleSentencesText{} unique attribute sentences. While these sentences are syntactically distinct from each other, semantically they can be very close. To improve the performance of the method proposed in this thesis, similar sentences are clustered. First, all sentences are embedded. Then, sentences with a cosine similarity higher than \num{\minCosineSimilarity} are clustered together while ensuring that every sentence is in exactly one cluster. The clustering is performed separately for style and knowledge sentences. From the resulting \num{\numClustersStyle} style clusters and \num{\numClustersKnowledge} knowledge clusters, the best ones are selected in multiple steps until the final interpretable attribute vector with \num{\styleVectorSize} dimensions is created, where each dimension corresponds to one cluster. The first \num{\numTargetPrompts} dimensions of the attribute vector are selected separately and correspond to the targeted prompts presented in Section~\ref{sec:approach:attributeSentenceGeneration}.}%
  \label{fig:clustering}
\end{figure}

\subsubsection{Selection of target attributes}
\label{sec:experiments:setup:selection:targetAttributes}
Following the work of \citet{patelLearningInterpretableStyle2023}, the first \num{\numTargetPrompts} dimensions of the attribute vector will be selected to correspond to the \num{\numTargetPrompts} target prompts, which were described in Section~\ref{sec:approach:attributeSentenceGeneration}.
This is done to ensure that the attribute vector has a robust foundation on some features that are manually selected for being relevant for stylistic research. The ability to automatically create most of the attribute vector is not significantly reduced since the target attributes only account for around \SI{10}{\percent} of its size.

These attributes are found by creating sentences of the form \enquote{The author uses <target>} and embedding them using the same embedding model that has been used to embed the attribute sentences, the all-MiniLM-L12-v2 model.
Afterward, for each sentence, the style cluster with the highest cosine similarity is found. This cluster is then one of the target attributes of the vector.

% TODO: write better?
The knowledge clusters are not taken into account during this selection. Since all style sentences that are produced by the targeted prompts can not be part of any knowledge cluster, any knowledge cluster that has a very high cosine similarity would be an unwanted cluster that should not be chosen at this point.

% TODO: compare to what patel et al. have done?

\subsubsection{Filtering out attributes that occur too frequently}
\label{sec:experiments:setup:selection:filteringOccurance}
An important characteristic of all attributes that are part of the attribute vector is that they are as meaningful as possible and help to distinguish between different groups. To ensure this requirement is met, attributes that describe texts of a too large portion of groups are removed. Based upon the work of \citet{patelLearningInterpretableStyle2023}, the maximum number of groups that can be described by an attribute cluster is \SI{60}{\percent}.

At the same time, the number of clusters that are taken into account for the next selection steps has to be reduced because these steps have a quadratic memory requirement. To quickly reduce the number of clusters, the number of times the cluster was used to describe answers is taken into account. The required number of times used is increased until the resulting number of clusters is small enough. This way, the largest clusters that are not used to describe too many groups are used for the next steps.

To determine how many times a cluster was used to describe an answer, each sentence is only counted once per answer. Multiple prompts could produce the same attribute sentence to describe an answer; counting all of them would however distort the number of usages of the clusters.

\subsubsection{Removing too similar Attributes}
\label{sec:experiments:setup:selection:removeSimilar}
To ensure that the attribute vector covers a wide range of attributes, it is important that the attributes do not cover too similar topics. This is achieved by ensuring that the clusters have a maximum cosine similarity of \num{\maxCosineSimilarity} to each other. This process works by ordering the clusters by occurrence, selecting them one after the other, and deleting all clusters that are too close to the ones that have already been selected.
The cosine similarity is computed between the embeddings of the attribute sentences that are closest to the center of the clusters.

This selection step follows the work of \citet{patelLearningInterpretableStyle2023}. However, they used a maximum cosine similarity of \num{0.8} between the embeddings of sentences. The lower value of \num{\maxCosineSimilarity} was chosen for two reasons. For one, sentences which embeddings have a higher cosine similarity than \num{\minCosineSimilarity} are clustered together as manual evaluation has shown that they have practically the same meaning. Because of this, a maximum cosine similarity of \num{0.8} would not have had very much effect. Additionally, since filtering out clusters which meaning is too close to each other has the effect of a more diverse attribute vector, a lower maximum cosine similarity is desirable as well. The data has shown that a lower number than \num{\minCosineSimilarity} filters out too many clusters and possibly reduces the quality of the attribute vector. % TODO: the data has shown? bessere formulierung?


\subsubsection{Final Selection} % TODO: better name?
\label{sec:experiments:setup:selection:finalSelection}
The first \num{\numTargetPrompts} dimensions of the attribute vector are the target attributes that were selected in Section~\ref{sec:experiments:setup:selection:targetAttributes}. The next \num{\minNumKnowledgePrompts} dimensions are knowledge clusters, that is, clusters of sentences that were produced by knowledge prompts. The rest of the dimensions are selected in this step to be the ones with the highest difference in agreement levels.

A difference in agreement levels means that the range between responses that fit the attribute well and those that do not is as wide as possible. This further increases the likelihood that the attributes are good at distinguishing between different responses and groups.

% TODO: complete code
% \lstinputlisting[language=Python, caption={Pseudo code for the final cluster selection step.}]{content/05-experiments/final-cluster-selection.py}

First, the similarity between all clusters that remain after the first selection steps is computed. Afterward, for each cluster and each answer, the average similarity to all other clusters that have been used to describe that answer is computed. The result is a rough estimation of the actual similarity between the current attribute sentence and the answer.

Subsequently, the range between the similarity of the most and least similar answers is computed, and the clusters with the highest difference are selected. With the process, the probability of attributes holding meaningful information to distinguish between different groups is increased.

\subsection{\acf{sfam}}
\label{sec:experiments:setup:sfam}
For the creation of the attribute vector, this thesis utilizes the \acl{sfam} proposed by \citet{patelLearningInterpretableStyle2023}. There are however two significant changes in comparison to the original model. For one, DeBERTa-V3-base\footnote{\url{https://huggingface.co/microsoft/deberta-v3-base}} (\cite{he2021deberta}) was used as the base model instead of t5-base. Furthermore, the final output is produced with a regression head with a single output instead of a binary classification head.
% TODO: explain why not deberta-large?
% TODO: what hyperparameters were used?

The final layer of the model proposed by \citet{patelLearningInterpretableStyle2023} has to output neurons; one for positive prediction and one for negative prediction. To get the agreement score, a softmax over the two values is used. In contrast to that, the method presented by this thesis uses a final layer with just one output neuron which has a sigmoid activation function. Therefore, the output of the model is the agreement score without any additional computation step.

The input to the model is truncated to 512 tokens following the work of \citet{patelLearningInterpretableStyle2023}.

\acs{sfam} is trained on the synthetic dataset that was described in the previous sections. The training data is balanced and consists of attribute sentences with one answer from the Stack Exchange Dataset (see Section~\ref{sec:datasets:stackex}), which fits the attribute sentence as a positive sample and one answer which does not fit the sentence as a negative sample. The input to the model is a string of the form \enquote{\(\{\{a\}\}~|||~\{\{t\}\}\)}, where \(a\) is the attribute sentence and \(t\) is the text that is the positive or negative sample.

The selection of the attribute sentences and answers used for training is similar to the process described in Section~\ref{sec:experiments:setup:selection:finalSelection} and differs from \citet{patelLearningInterpretableStyle2023}. They select sentences for training at random and choose a text that is described with the sentence in the synthetic dataset as a positive sample. A negative sample is selected by sampling a text that is described by the sentence with the largest cosine distance to the current sentence.
The potential problem with this process is that the annotation in the synthetic dataset is not perfect and it is possible that the text that are selected as a positive sample do not actually correspond to the sentence.

The method proposed in this thesis instead orders all answers by a heuristic similarity to the sentence and selects one of the most similar to a positive sample and one of the most dissimilar as a negative sample. For each sentence, first the heuristic similarity to each text is computed. This is done by computing the similarity to all attribute vector dimensions that are used to describe the text in the synthetic dataset. The average of these similarities serves as an approximation for the actual similarity.

Subsequently, the answers are ordered by their heuristic similarity to the attribute sentence. The positive training sample is the most similar text where the sentence was used to describe it in the synthetic dataset. The negative training sample is the most dissimilar sentence that was not described by the sentence. If a candidate for the positive sample is not in the top \sfamDataTopPercentText{}, the sentence is not used for the \ac{sfam} training dataset. % TODO: reason
Since the synthetic dataset consists of \numStyleSentencesText{} attribute sentences and the \ac{sfam} training dataset consists of only \num{\sfamTrainingDataSize} samples, it is no problem to discard sentences where the positive sample does not have a sufficiently high heuristic similarity.

The validation and test datasets are produced with the same method. However, while \citet{patelLearningInterpretableStyle2023} sample distinct sets of sentences for the training, validation, and test datasets, for this thesis the input texts themselves are assigned to the different dataset categories. The attribute sentences produced from these texts are also largely distinct, also this is not enforced and simply a side effect of the \ac{llm} using different formulations for the same concepts. % (see Figure~\ref{fig:sfamDatasetSplit}).
Because of this, \ac{sfam} is tested not only on unseen attribute sentences but on unseen texts as well.

% TODO: improve?
% \begin{figure}[ht]
%   \begin{center}
%     \input{figures/python/sfam_dataset_split.pgf}
%   \end{center}
%   \caption{TODO:}%
%   \label{fig:sfamDatasetSplit}
% \end{figure}


\subsection{\acf{lisa} Model}
\label{sec:experiments:setup:lisa}
While \ac{sfam} is sufficient to produce the interpretable attribute vectors that are the goal of this thesis, it has the disadvantage that a forward pass is required for each individual of the \num{\styleVectorSize} dimensions of the attribute vector. To improve the runtime performance, this thesis implements the \ac{lisa} model following the work of \citet{patelLearningInterpretableStyle2023}.
% TODO: what hyperparameters were used?

The model is based upon the newer and more capable DeBERTa-V3-large\footnote{\url{https://huggingface.co/microsoft/deberta-v3-large}} model instead of t5-base. Attached to the model is a regression head that directly outputs the attribute vector in one forward pass. Unlike \citet{patelLearningInterpretableStyle2023}, who truncate output values that are not between \num{0} and \num{1}, the model implemented for this thesis has a sigmoid activation function in the final layer, so that the output values of the model are always in the correct range. This has the advantage that the model is less likely to predict extreme agreement scores for attributes. This corresponds to the reality, because most of the attributes correspond to the texts mostly, but not completely.

\ac{lisa} is trained on the attribute vectors that were created with \ac{sfam}. For this thesis, a training dataset with \num{\lisaTrainingDataSize} samples, a validation dataset with \num{\lisaValDataSize} samples, and a test dataset with \num{\lisaTestDataSize} samples were used.


\subsection{Embedding Model}
\label{sec:experiments:setup:embedder}
While the attribute vector produced by \ac{lisa} is interpretable, it is not suited for the task of group-membership detection. Because the dimensions of the vector are not normalized, attribute vectors of different texts can not be meaningfully compared to each other. To fix this problem, this thesis implements an embedding head for the \ac{lisa} model following the work of \citet{patelLearningInterpretableStyle2023}.

The embedding model consists of two fully connected layers with a ReLU activation function in between. The input is the attribute vector, and the output is an embedding with \num{\styleEmbeddingSize} dimensions. The number of embedding dimensions follows previous work by \citet{patelLearningInterpretableStyle2023}. The model was trained using a triplet loss with a contrastive learning objective\footnote{\url{https://docs.pytorch.org/docs/stable/generated/torch.nn.TripletMarginLoss.html}} (\cite{NEURIPS2020_d89a66c7}) where the distance is measured as the Euclidean distance with a margin of \num{5.0}.

% TODO:
% For the training process, a learning rate of \ldots and a weight decay of \ldots was chosen. These values were identified as the best ones with a hyperparameter search using the library optuna\footnote{\url{https://optuna.org/}}.
Each training sample consists of three attribute vectors, two of the same group and one of a different group. The model creates the embeddings for all three vectors. The training goal is that the anchor embedding is closer to the positive embedding (the same group) than the negative embedding (the different group). The training, validation, and test datasets are the same as the ones used for the \ac{lisa} model.


\subsection{Steering}
\label{sec:experiments:setup:steering}
After introducing a method to create interpretable attribute vectors and using them for group-membership detection, the next step is to steer explanations generated by \aclp{llm} towards the style and background knowledge of specific groups. The previous parts of this thesis will be used to create steering methods as well as evaluate them.

This thesis proposes two kinds of steering methods. Firstly, the system prompt will be changed in different ways to instruct the \ac{llm} to change its answer to be appropriate for a specific group. Secondly, the model will be steered with a method based upon the work of \citet{konenStyleVectorsSteering2024,turnerActivationAdditionSteering2024,rimsky-etal-2024-steering}. This approach manipulates the activation space of the model to steer the text generation towards specific concepts that correspond to dimensions of the interpretable attribute vector. The activation steering will be combined with the different prompt steering methods to provide a complete comparison.
All methods will be compared against a baseline text generation that is unsteered.
For all steering methods, the model Llama-3.2-3B-instruct\footnote{\url{https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct}} was used.

\subsubsection{Prompt Steering}
\label{sec:experiments:setup:steering:prompt}
The first and simplest way to change the system prompt is to instruct the model to write an explanation for a specified group with the \textbf{Prompt Group Steering} method. In this case, the system prompt includes the following sentence:
\begin{quote} % TODO: include final steering prompt
  You are an author who writes a helpful explanation for a group of <group>.
\end{quote}
While this method can not be influenced regarding the direction or strength of the steering effect, it is very easy to implement. Because this method does not make use of any of the approaches presented in this thesis, it serves as a baseline steering method of what is possible with simple prompt engineering.

The second approach is the \textbf{Prompt Attribute Steering} method. Here, the most important dimensions of the attribute vector of a group are used to steer the model. The most important attributes are extracted from the synthetic dataset (see Section~\ref{sec:approach:attributeSentenceGeneration} and Section~\ref{sec:experiments:setup:sentenceGeneration}) by computing the point-biserial\footnote{\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pointbiserialr.html}} correlation for each dimension of the interpretable attribute vector in relation with the groups.

For the steering process, the most important \num{\numImportantAttributesStyle} style attributes and \num{\numImportantAttributesKnowledge} knowledge attributes are used in the prompt. This number was chosen because more attributes would, for one, make it more difficult for the \ac{llm} to steer its generation towards all mentioned attributes, and on the other hand, make it likelier that the attributes are not very important for the group. Related to that, the number of knowledge attributes has to be lower than the number of style attributes because only \num{\minNumKnowledgePrompts} out of \num{\styleVectorSize} dimensions of the interpretable attribute vector are knowledge attributes. Using a higher number would therefore increase the chance of an irrelevant knowledge attribute being part of the system prompt.

For the attributes to be able to be effectively used in the system prompt, they can not be of the form \enquote{The author is \ldots}. Instead, the style attributes are automatically rewritten to address the model directly, as this role-playing is a strength of modern \aclp{llm}. % TODO: citation
Thus, the attribute sentences will have the form \enquote{You are \ldots}.
However, the knowledge attributes cannot be written this way because they describe the knowledge of the audience, not the knowledge of the model. Therefore, the system prompt includes an instruction to write for a group of people with some special knowledge. Afterward, the knowledge attributes are written in the form \enquote{They are \ldots}. The attributes are subsequently inserted into the following template:
\begin{quote} % TODO: include final steering prompt
  You are an author who writes a short and helpful explanation. \newline
  <rephrased style attributes> \\[5pt]
  You write for a group of people who have the following background knowledge and experience: \newline
  <rephrased knowledge attributes>
\end{quote}

The final \textbf{Prompt Group Attribute Steering} method will combine the two previous methods and include both the group name and the most important attribute sentences in the system prompt, which results in the following template:

\begin{quote} % TODO: include final steering prompt
  You are an author who writes a helpful explanation for a group of <group>. \newline
  <rephrased style attributes> \\[5pt]
  You write for a group of people who have the following background knowledge and experience: \newline
  <rephrased knowledge attributes>
\end{quote}

%  experiment, not setup
% Per steering method, the \ac{llm} will be prompted for each of the \num{\steeringDataSize} questions in the steering dataset (see Section~\ref{sec:datasets:steering}) and each of the \num{\numGroups} groups to generate a group-specific explanation.
% This explanation is then embedded with the embedding model presented in Section~\ref{sec:experiments:setup:embedder}.
% To evaluate the steering effect

\subsubsection{Activation Steering}
\label{sec:experiments:setup:steering:activation}
While steering \iac{llm} by changing its system prompt is a reliable method with good results, it has the disadvantage that it is difficult to change the strength of the steering effect. Since the values of the interpretable attribute vector represent how strongly an attribute matches a text, it would be useful to incorporate this information into the steering.

The activation steering approach first extracts the activation vectors at each layer of the model that correspond to each dimension of the attribute vector. The extraction is done in one forward pass and is based upon the work of \citet{konenStyleVectorsSteering2024}. % turnerActivationAdditionSteering2024,rimsky-etal-2024-steering
Afterward, the activation vectors are combined weighted by their importance according to the interpretable attribute vector. The result steering vector is added at the layer of the model from which it was extracted during the explanation generation. It is scaled with a factor \(\lambda\) that determines the strength of the steering effect.

To extract the activation vectors, firstly, the attribute vector dimensions are converted to sentences of the form \enquote{You use \ldots} instead of \enquote{The author uses \ldots}, similar to the prompt attribute steering process described in the previous section. This converted attribute is part of the system prompt that was described in Section~\ref{sec:experiments:setup:steering:prompt}. The \ac{llm} is then prompted without any further instruction. To generate the token during the forward pass while following the attribute sentence, the model has to incorporate the meaning of the sentence. \citet{konenStyleVectorsSteering2024} discovered that this meaning is represented in the activation vectors at each layer of the model, which are extracted and saved. The token that was generated by the model during the forward pass is not relevant.

To create the steering vector, the activation vectors for each dimension of the attribute vector are combined. The basis for this combination is the median group vector from all answers of the synthetic dataset presented in Section~\ref{sec:approach:attributeSentenceGeneration}. For all dimensions that do not belong to the \num{\numImportantAttributesStyle} most important style attributes or \num{\numImportantAttributesKnowledge} most important knowledge attributes, the value is set to negative infinity. Then, the softmax of the attribute vector is computed. The softmax is then used to weigh the activation vectors of all layers. After adding the weighted vectors together, the result is a steering vector for the group for each layer of the model. This is repeated for all groups.
