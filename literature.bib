@online{alhafniPersonalizedTextGeneration2024,
  title = {Personalized Text Generation with Fine-Grained Linguistic Control},
  author = {Alhafni, Bashar and Kulkarni, Vivek and Kumar, Dhruv and Raheja, Vipul},
  date = {2024-02-07},
  eprint = {2402.04914},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.04914},
  url = {http://arxiv.org/abs/2402.04914},
  urldate = {2024-09-28},
  abstract = {As the text generation capabilities of large language models become increasingly prominent, recent studies have focused on controlling particular aspects of the generated text to make it more personalized. However, most research on controllable text generation focuses on controlling the content or modeling specific high-level/coarse-grained attributes that reflect authors' writing styles, such as formality, domain, or sentiment. In this paper, we focus on controlling fine-grained attributes spanning multiple linguistic dimensions, such as lexical and syntactic attributes. We introduce a novel benchmark to train generative models and evaluate their ability to generate personalized text based on multiple fine-grained linguistic attributes. We systematically investigate the performance of various large language models on our benchmark and draw insights from the factors that impact their performance. We make our code, data, and pretrained models publicly available.},
  langid = {english},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Computation and Language,recommended},
  file = {C\:\\Users\\janek\\Zotero\\storage\\LK6K3YYG\\Alhafni et al. - 2024 - Personalized Text Generation with Fine-Grained Linguistic Control.pdf;C\:\\Users\\janek\\Zotero\\storage\\7FLRNLEZ\\2402.html}
}

@online{doddapaneniUserEmbeddingModel2024,
  title = {User Embedding Model for Personalized Language Prompting},
  author = {Doddapaneni, Sumanth and Sayana, Krishna and Jash, Ambarish and Sodhi, Sukhdeep and Kuzmin, Dima},
  date = {2024-01-09},
  eprint = {2401.04858},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2401.04858},
  urldate = {2024-09-27},
  abstract = {Modeling long histories plays a pivotal role in enhancing recommendation systems, allowing to capture user's evolving preferences, resulting in more precise and personalized recommendations. In this study we tackle the challenges of modeling long user histories for preference understanding in natural language. Specifically, we introduce a new User Embedding Module (UEM) that efficiently processes user history in free-form text by compressing and representing them as embeddings, to use them as soft prompts to a LM. Our experiments demonstrate the superior capability of this approach in handling significantly longer histories compared to conventional text based prompting methods, yielding substantial improvements in predictive performance. The main contribution of this research is to demonstrate the ability to bias language models with user signals represented as embeddings.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,recommended},
  file = {C\:\\Users\\janek\\Zotero\\storage\\PBA2TXBM\\Doddapaneni et al. - 2024 - User Embedding Model for Personalized Language Prompting.pdf;C\:\\Users\\janek\\Zotero\\storage\\ZDG4PZFU\\2401.html}
}

@article{gilardiChatGPTOutperformsCrowdworkers2023,
  title = {{{ChatGPT}} Outperforms Crowd-Workers for Text-Annotation Tasks},
  author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Maël},
  date = {2023-07-25},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci.},
  volume = {120},
  number = {30},
  eprint = {2303.15056},
  eprinttype = {arXiv},
  eprintclass = {cs},
  pages = {e2305016120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2305016120},
  url = {http://arxiv.org/abs/2303.15056},
  urldate = {2024-09-29},
  abstract = {Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {C\:\\Users\\janek\\Zotero\\storage\\I359IUK3\\Gilardi et al. - 2023 - ChatGPT outperforms crowd-workers for text-annotation tasks.pdf;C\:\\Users\\janek\\Zotero\\storage\\5WURXX9I\\2303.html}
}

@online{honovichUnnaturalInstructionsTuning2022,
  title = {Unnatural Instructions: Tuning Language Models with (Almost) {{No}} Human Labor},
  shorttitle = {Unnatural Instructions},
  author = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
  date = {2022-12-19},
  eprint = {2212.09689},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.09689},
  url = {http://arxiv.org/abs/2212.09689},
  urldate = {2024-09-29},
  abstract = {Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\janek\\Zotero\\storage\\Q92JKR3J\\Honovich et al. - 2022 - Unnatural instructions tuning language models with (almost) No human labor.pdf;C\:\\Users\\janek\\Zotero\\storage\\865QE4HB\\2212.html}
}

@online{huangLargeLanguageModels2022,
  title = {Large Language Models Can Self-Improve},
  author = {Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  date = {2022-10-25},
  eprint = {2210.11610},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.11610},
  url = {http://arxiv.org/abs/2210.11610},
  urldate = {2024-09-29},
  abstract = {Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate "high-confidence" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4\%-{$>$}82.1\% on GSM8K, 78.2\%-{$>$}83.0\% on DROP, 90.0\%-{$>$}94.4\% on OpenBookQA, and 63.4\%-{$>$}67.9\% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\janek\\Zotero\\storage\\ZWTBE7SI\\Huang et al. - 2022 - Large language models can self-improve.pdf;C\:\\Users\\janek\\Zotero\\storage\\ML59CG85\\2210.html}
}

@online{konenStyleVectorsSteering2024,
  title = {Style Vectors for Steering Generative Large Language Model},
  author = {Konen, Kai and Jentzsch, Sophie and Diallo, Diaoulé and Schütt, Peer and Bensch, Oliver and Baff, Roxanne El and Opitz, Dominik and Hecking, Tobias},
  date = {2024-02-02},
  eprint = {2402.01618},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.01618},
  url = {http://arxiv.org/abs/2402.01618},
  urldate = {2024-09-28},
  abstract = {This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.},
  langid = {english},
  pubstate = {prepublished},
  version = {1},
  keywords = {Computer Science - Computation and Language,reading complete,recommended},
  file = {C\:\\Users\\janek\\Zotero\\storage\\U5R9YU3J\\Konen et al. - 2024 - Style vectors for steering generative large language model.pdf;C\:\\Users\\janek\\Zotero\\storage\\MMSHLJ2Z\\2402.html}
}

@online{patelLearningInterpretableStyle2023,
  title = {Learning Interpretable Style Embeddings via Prompting {{LLMs}}},
  author = {Patel, Ajay and Rao, Delip and Kothary, Ansh and McKeown, Kathleen},
  date = {2023-10-09},
  eprint = {2305.12696},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.12696},
  url = {http://arxiv.org/abs/2305.12696},
  urldate = {2024-09-27},
  abstract = {Style representation learning builds content-independent representations of author style in text. Stylometry, the analysis of style in text, is often performed by expert forensic linguists and no large dataset of stylometric annotations exists for training. Current style representation learning uses neural methods to disentangle style from content to create style vectors, however, these approaches result in uninterpretable representations, complicating their usage in downstream applications like authorship attribution where auditing and explainability is critical. In this work, we use prompting to perform stylometry on a large number of texts to create a synthetic dataset and train human-interpretable style representations we call LISA embeddings. We release our synthetic stylometry dataset and our interpretable style models as resources.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,reading complete,recommended},
  file = {C\:\\Users\\janek\\Zotero\\storage\\CIUFXHT9\\Patel et al. - 2023 - Learning Interpretable Style Embeddings via Prompting LLMs.pdf;C\:\\Users\\janek\\Zotero\\storage\\B383WVHI\\2305.html}
}

@online{subramaniExtractingLatentSteering2022,
  title = {Extracting Latent Steering Vectors from Pretrained Language Models},
  author = {Subramani, Nishant and Suresh, Nivedita and Peters, Matthew E.},
  date = {2022-05-10},
  eprint = {2205.05124},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.05124},
  url = {http://arxiv.org/abs/2205.05124},
  urldate = {2024-09-27},
  abstract = {Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly ({$>$} 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,reading complete,recommended},
  file = {C\:\\Users\\janek\\Zotero\\storage\\L89NKEG9\\Subramani et al. - 2022 - Extracting Latent Steering Vectors from Pretrained Language Models.pdf;C\:\\Users\\janek\\Zotero\\storage\\8D3HFBUF\\2205.html}
}

@online{turnerActivationAdditionSteering2024,
  title = {Activation Addition: Steering Language Models without Optimization},
  shorttitle = {Activation Addition},
  author = {Turner, Alexander Matt and Thiergart, Lisa and Leech, Gavin and Udell, David and Vazquez, Juan J. and Mini, Ulisse and MacDiarmid, Monte},
  date = {2024-06-04},
  eprint = {2308.10248},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2308.10248},
  urldate = {2024-09-27},
  abstract = {Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering and guided decoding. We instead investigate activation engineering: modifying activations at inference-time to predictably alter model behavior. We bias the forward pass with a 'steering vector' implicitly specified through natural language. Past work learned these steering vectors; our Activation Addition (ActAdd) method instead computes them by taking activation differences resulting from pairs of prompts. We demonstrate ActAdd on a range of LLMs (LLaMA-3, OPT, GPT-2, and GPT-J), obtaining SOTA on detoxification and negative-to-positive sentiment control. Our approach yields inference-time control over high-level properties of output like topic and sentiment while preserving performance on off-target tasks. ActAdd takes far less compute and implementation effort than finetuning or RLHF, allows users control through natural language, and its computational overhead (as a fraction of inference time) appears stable or improving over increasing model size.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,reading complete,recommended},
  file = {C\:\\Users\\janek\\Zotero\\storage\\UWUCEGH9\\Turner et al. - 2024 - Activation Addition Steering Language Models Without Optimization.pdf;C\:\\Users\\janek\\Zotero\\storage\\D8I89JDU\\2308.html}
}

@online{wangSelfinstructAligningLanguage2023,
  title = {Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  shorttitle = {Self-Instruct},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  date = {2023-05-25},
  eprint = {2212.10560},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.10560},
  url = {http://arxiv.org/abs/2212.10560},
  urldate = {2024-09-29},
  abstract = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
  langid = {english},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\janek\Zotero\storage\AHZEK9UN\Wang et al. - 2023 - Self-instruct aligning language models with self-generated instructions.pdf}
}

@inproceedings{wegmann-nguyen-2021-capture,
  title = {Does It Capture {{STEL}}? {{A}} Modular, Similarity-Based Linguistic Style Evaluation Framework},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  author = {Wegmann, Anna and Nguyen, Dong},
  editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  date = {2021-11},
  pages = {7109--7130},
  publisher = {Association for Computational Linguistics},
  location = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.569},
  url = {https://aclanthology.org/2021.emnlp-main.569},
  abstract = {Style is an integral part of natural language. However, evaluation methods for style measures are rare, often task-specific and usually do not control for content. We propose the modular, fine-grained and content-controlled similarity-based STyle EvaLuation framework (STEL) to test the performance of any model that can compare two sentences on style. We illustrate STEL with two general dimensions of style (formal/informal and simple/complex) as well as two specific characteristics of style (contrac'tion and numb3r substitution). We find that BERT-based methods outperform simple versions of commonly used style measures like 3-grams, punctuation frequency and LIWC-based approaches. We invite the addition of further tasks and task instances to STEL and hope to facilitate the improvement of style-sensitive measures.},
  langid = {english},
  file = {C:\Users\janek\Zotero\storage\RJYR7QVM\Wegmann und Nguyen - 2021 - Does It Capture S℡ A Modular, Similarity-based Linguistic Style Evaluation Framework.pdf}
}
