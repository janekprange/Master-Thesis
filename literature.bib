@misc{alhafniPersonalizedTextGeneration2024,
  title = {Personalized Text Generation with Fine-Grained Linguistic Control},
  author = {Alhafni, Bashar and Kulkarni, Vivek and Kumar, Dhruv and Raheja, Vipul},
  year = {2024},
  month = feb,
  number = {arXiv:2402.04914},
  eprint = {2402.04914},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.04914},
  urldate = {2024-09-28},
  abstract = {As the text generation capabilities of large language models become increasingly prominent, recent studies have focused on controlling particular aspects of the generated text to make it more personalized. However, most research on controllable text generation focuses on controlling the content or modeling specific high-level/coarse-grained attributes that reflect authors' writing styles, such as formality, domain, or sentiment. In this paper, we focus on controlling fine-grained attributes spanning multiple linguistic dimensions, such as lexical and syntactic attributes. We introduce a novel benchmark to train generative models and evaluate their ability to generate personalized text based on multiple fine-grained linguistic attributes. We systematically investigate the performance of various large language models on our benchmark and draw insights from the factors that impact their performance. We make our code, data, and pretrained models publicly available.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,recommended},
  file = {C\:\\Users\\janek\\Zotero\\storage\\LK6K3YYG\\Alhafni et al. - 2024 - Personalized Text Generation with Fine-Grained Linguistic Control.pdf;C\:\\Users\\janek\\Zotero\\storage\\7FLRNLEZ\\2402.html}
}

@book{birdNaturalLanguageProcessing2009,
  title = {Natural Language Processing with {{Python}}},
  author = {Bird, Steven and Klein, Ewan and Loper, Edward},
  year = {2009},
  edition = {1st ed},
  publisher = {O'Reilly},
  address = {Beijing ; Cambridge [Mass.]},
  abstract = {This is an introduction to natural language processing, which supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation},
  isbn = {978-0-596-51649-9},
  lccn = {QA76.73.P98 B57 2009},
  keywords = {Natural language processing (Computer science),programming_library,Python (Computer program language),Python <Programmiersprache>,Sprachverarbeitung},
  annotation = {OCLC: ocn301885973}
}

@misc{doddapaneniUserEmbeddingModel2024,
  title = {User Embedding Model for Personalized Language Prompting},
  author = {Doddapaneni, Sumanth and Sayana, Krishna and Jash, Ambarish and Sodhi, Sukhdeep and Kuzmin, Dima},
  year = {2024},
  month = jan,
  number = {arXiv:2401.04858},
  eprint = {2401.04858},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-27},
  abstract = {Modeling long histories plays a pivotal role in enhancing recommendation systems, allowing to capture user's evolving preferences, resulting in more precise and personalized recommendations. In this study we tackle the challenges of modeling long user histories for preference understanding in natural language. Specifically, we introduce a new User Embedding Module (UEM) that efficiently processes user history in free-form text by compressing and representing them as embeddings, to use them as soft prompts to a LM. Our experiments demonstrate the superior capability of this approach in handling significantly longer histories compared to conventional text based prompting methods, yielding substantial improvements in predictive performance. The main contribution of this research is to demonstrate the ability to bias language models with user signals represented as embeddings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning,recommended},
  file = {C\:\\Users\\janek\\Zotero\\storage\\PBA2TXBM\\Doddapaneni et al. - 2024 - User Embedding Model for Personalized Language Prompting.pdf;C\:\\Users\\janek\\Zotero\\storage\\ZDG4PZFU\\2401.html}
}

@article{gilardiChatGPTOutperformsCrowdworkers2023,
  title = {{{ChatGPT}} Outperforms Crowd-Workers for Text-Annotation Tasks},
  author = {Gilardi, Fabrizio and Alizadeh, Meysam and Kubli, Ma{\"e}l},
  year = {2023},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {30},
  eprint = {2303.15056},
  primaryclass = {cs},
  pages = {e2305016120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2305016120},
  urldate = {2024-09-29},
  abstract = {Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than \$0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {C\:\\Users\\janek\\Zotero\\storage\\I359IUK3\\Gilardi et al. - 2023 - ChatGPT outperforms crowd-workers for text-annotation tasks.pdf;C\:\\Users\\janek\\Zotero\\storage\\5WURXX9I\\2303.html}
}

@article{harris2020array,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and {van der Walt}, St{\'e}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and {van Kerkwijk}, Marten H. and Brett, Matthew and Haldane, Allan and {del R{\'i}o}, Jaime Fern{\'a}ndez and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  month = sep,
  journal = {Nature},
  volume = {585},
  number = {7825},
  pages = {357--362},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1038/s41586-020-2649-2},
  langid = {english},
  keywords = {programming_library}
}

@misc{honovichUnnaturalInstructionsTuning2022,
  title = {Unnatural Instructions: Tuning Language Models with (Almost) {{No}} Human Labor},
  shorttitle = {Unnatural Instructions},
  author = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09689},
  eprint = {2212.09689},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.09689},
  urldate = {2024-09-29},
  abstract = {Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\janek\\Zotero\\storage\\Q92JKR3J\\Honovich et al. - 2022 - Unnatural instructions tuning language models with (almost) No human labor.pdf;C\:\\Users\\janek\\Zotero\\storage\\865QE4HB\\2212.html}
}

@misc{huangLargeLanguageModels2022,
  title = {Large Language Models Can Self-Improve},
  author = {Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  year = {2022},
  month = oct,
  number = {arXiv:2210.11610},
  eprint = {2210.11610},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.11610},
  urldate = {2024-09-29},
  abstract = {Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate "high-confidence" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4\%-{$>$}82.1\% on GSM8K, 78.2\%-{$>$}83.0\% on DROP, 90.0\%-{$>$}94.4\% on OpenBookQA, and 63.4\%-{$>$}67.9\% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\janek\\Zotero\\storage\\ZWTBE7SI\\Huang et al. - 2022 - Large language models can self-improve.pdf;C\:\\Users\\janek\\Zotero\\storage\\ML59CG85\\2210.html}
}

@misc{konenStyleVectorsSteering2024,
  title = {Style Vectors for Steering Generative Large Language Model},
  author = {Konen, Kai and Jentzsch, Sophie and Diallo, Diaoul{\'e} and Sch{\"u}tt, Peer and Bensch, Oliver and Baff, Roxanne El and Opitz, Dominik and Hecking, Tobias},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01618},
  eprint = {2402.01618},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.01618},
  urldate = {2024-09-28},
  abstract = {This research explores strategies for steering the output of large language models (LLMs) towards specific styles, such as sentiment, emotion, or writing style, by adding style vectors to the activations of hidden layers during text generation. We show that style vectors can be simply computed from recorded layer activations for input texts in a specific style in contrast to more complex training-based approaches. Through a series of experiments, we demonstrate the effectiveness of activation engineering using such style vectors to influence the style of generated text in a nuanced and parameterisable way, distinguishing it from prompt engineering. The presented research constitutes a significant step towards developing more adaptive and effective AI-empowered interactive systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,reading complete,recommended},
  file = {C\:\\Users\\janek\\Zotero\\storage\\U5R9YU3J\\Konen et al. - 2024 - Style vectors for steering generative large language model.pdf;C\:\\Users\\janek\\Zotero\\storage\\MMSHLJ2Z\\2402.html}
}

@inproceedings{mckinney-proc-scipy-2010,
  title = {Data {{Structures}} for {{Statistical Computing}} in {{Python}}},
  booktitle = {Proceedings of the 9th {{Python}} in {{Science Conference}}},
  author = {McKinney, Wes},
  editor = {{van der Walt}, St{\'e}fan and Millman, Jarrod},
  year = {2010},
  pages = {56--61},
  doi = {10.25080/Majora-92bf1922-00a},
  keywords = {programming_library}
}

@misc{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K{\"o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  month = dec,
  number = {arXiv:1912.01703},
  eprint = {1912.01703},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1912.01703},
  urldate = {2024-11-04},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Mathematical Software,programming_library,Statistics - Machine Learning},
  file = {C\:\\Users\\janek\\Zotero\\storage\\WXUSI4PU\\Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Deep Learning Library.pdf;C\:\\Users\\janek\\Zotero\\storage\\J6JCU35G\\1912.html}
}

@misc{patelLearningInterpretableStyle2023,
  title = {Learning Interpretable Style Embeddings via Prompting {{LLMs}}},
  author = {Patel, Ajay and Rao, Delip and Kothary, Ansh and McKeown, Kathleen},
  year = {2023},
  month = oct,
  number = {arXiv:2305.12696},
  eprint = {2305.12696},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.12696},
  urldate = {2024-09-27},
  abstract = {Style representation learning builds content-independent representations of author style in text. Stylometry, the analysis of style in text, is often performed by expert forensic linguists and no large dataset of stylometric annotations exists for training. Current style representation learning uses neural methods to disentangle style from content to create style vectors, however, these approaches result in uninterpretable representations, complicating their usage in downstream applications like authorship attribution where auditing and explainability is critical. In this work, we use prompting to perform stylometry on a large number of texts to create a synthetic dataset and train human-interpretable style representations we call LISA embeddings. We release our synthetic stylometry dataset and our interpretable style models as resources.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,reading complete,recommended},
  file = {C\:\\Users\\janek\\Zotero\\storage\\CIUFXHT9\\Patel et al. - 2023 - Learning Interpretable Style Embeddings via Prompting LLMs.pdf;C\:\\Users\\janek\\Zotero\\storage\\B383WVHI\\2305.html}
}

@misc{reback2020pandas,
  title = {Pandas-Dev/Pandas: {{Pandas}}},
  author = {pandas development {team}, The},
  year = {2020},
  month = feb,
  doi = {10.5281/zenodo.3509134},
  howpublished = {Zenodo},
  keywords = {programming_library}
}

@misc{reimersSentenceBERTSentenceEmbeddings2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10084},
  eprint = {1908.10084},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1908.10084},
  urldate = {2024-11-04},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,programming_library},
  file = {C\:\\Users\\janek\\Zotero\\storage\\JNXUC4XS\\Reimers und Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese BERT-Networks.pdf;C\:\\Users\\janek\\Zotero\\storage\\GG825WFB\\1908.html}
}

@misc{subramaniExtractingLatentSteering2022,
  title = {Extracting Latent Steering Vectors from Pretrained Language Models},
  author = {Subramani, Nishant and Suresh, Nivedita and Peters, Matthew E.},
  year = {2022},
  month = may,
  number = {arXiv:2205.05124},
  eprint = {2205.05124},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.05124},
  urldate = {2024-09-27},
  abstract = {Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly ({$>$} 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,reading complete,recommended},
  file = {C\:\\Users\\janek\\Zotero\\storage\\L89NKEG9\\Subramani et al. - 2022 - Extracting Latent Steering Vectors from Pretrained Language Models.pdf;C\:\\Users\\janek\\Zotero\\storage\\8D3HFBUF\\2205.html}
}

@misc{turnerActivationAdditionSteering2024,
  title = {Activation Addition: Steering Language Models without Optimization},
  shorttitle = {Activation Addition},
  author = {Turner, Alexander Matt and Thiergart, Lisa and Leech, Gavin and Udell, David and Vazquez, Juan J. and Mini, Ulisse and MacDiarmid, Monte},
  year = {2024},
  month = jun,
  number = {arXiv:2308.10248},
  eprint = {2308.10248},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-27},
  abstract = {Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering and guided decoding. We instead investigate activation engineering: modifying activations at inference-time to predictably alter model behavior. We bias the forward pass with a 'steering vector' implicitly specified through natural language. Past work learned these steering vectors; our Activation Addition (ActAdd) method instead computes them by taking activation differences resulting from pairs of prompts. We demonstrate ActAdd on a range of LLMs (LLaMA-3, OPT, GPT-2, and GPT-J), obtaining SOTA on detoxification and negative-to-positive sentiment control. Our approach yields inference-time control over high-level properties of output like topic and sentiment while preserving performance on off-target tasks. ActAdd takes far less compute and implementation effort than finetuning or RLHF, allows users control through natural language, and its computational overhead (as a fraction of inference time) appears stable or improving over increasing model size.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,reading complete,recommended},
  file = {C\:\\Users\\janek\\Zotero\\storage\\UWUCEGH9\\Turner et al. - 2024 - Activation Addition Steering Language Models Without Optimization.pdf;C\:\\Users\\janek\\Zotero\\storage\\D8I89JDU\\2308.html}
}

@misc{wangSelfinstructAligningLanguage2023,
  title = {Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  shorttitle = {Self-Instruct},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  year = {2023},
  month = may,
  number = {arXiv:2212.10560},
  eprint = {2212.10560},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2212.10560},
  urldate = {2024-09-29},
  abstract = {Large "instruction-tuned" language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C:\Users\janek\Zotero\storage\AHZEK9UN\Wang et al. - 2023 - Self-instruct aligning language models with self-generated instructions.pdf}
}

@inproceedings{wegmann-nguyen-2021-capture,
  title = {Does It Capture {{STEL}}? {{A}} Modular, Similarity-Based Linguistic Style Evaluation Framework},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  author = {Wegmann, Anna and Nguyen, Dong},
  editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  year = {2021},
  month = nov,
  pages = {7109--7130},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.569},
  abstract = {Style is an integral part of natural language. However, evaluation methods for style measures are rare, often task-specific and usually do not control for content. We propose the modular, fine-grained and content-controlled similarity-based STyle EvaLuation framework (STEL) to test the performance of any model that can compare two sentences on style. We illustrate STEL with two general dimensions of style (formal/informal and simple/complex) as well as two specific characteristics of style (contrac'tion and numb3r substitution). We find that BERT-based methods outperform simple versions of commonly used style measures like 3-grams, punctuation frequency and LIWC-based approaches. We invite the addition of further tasks and task instances to STEL and hope to facilitate the improvement of style-sensitive measures.},
  langid = {english},
  file = {C:\Users\janek\Zotero\storage\RJYR7QVM\Wegmann und Nguyen - 2021 - Does It Capture S℡ A Modular, Similarity-based Linguistic Style Evaluation Framework.pdf}
}

@misc{wolfHuggingFacesTransformersStateart2020,
  title = {{{HuggingFace}}'s {{Transformers}}: {{State-of-the-art Natural Language Processing}}},
  shorttitle = {{{HuggingFace}}'s {{Transformers}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2020},
  month = jul,
  number = {arXiv:1910.03771},
  eprint = {1910.03771},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1910.03771},
  urldate = {2024-11-04},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. {\textbackslash}textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. {\textbackslash}textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at {\textbackslash}url\{https://github.com/huggingface/transformers\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,programming_library},
  file = {C\:\\Users\\janek\\Zotero\\storage\\XHYNAELR\\Wolf et al. - 2020 - HuggingFace's Transformers State-of-the-art Natural Language Processing.pdf;C\:\\Users\\janek\\Zotero\\storage\\QFT8ZTJN\\1910.html}
}
