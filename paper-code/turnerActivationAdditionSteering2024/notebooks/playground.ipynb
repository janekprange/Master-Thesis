{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from IPython.core.getipython import get_ipython\n",
    "    get_ipython().run_line_magic(\"pip\", \"install transformers sentencepiece accelerate\")\n",
    "    get_ipython().run_line_magic(\"pip\", \"install git+https://github.com/UlisseMini/activation_additions_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import activation_additions as aa\n",
    "\n",
    "from typing import List, Dict, Union, Callable, Tuple\n",
    "from functools import partial\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from activation_additions.compat import ActivationAddition, get_x_vector, print_n_comparisons, pretty_print_completions, get_n_comparisons\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from functools import lru_cache\n",
    "from activation_additions.utils import colored_tokens\n",
    "from IPython.display import display, HTML\n",
    "from ipywidgets import interact, FloatSlider, IntSlider, Text, fixed\n",
    "from huggingface_hub import snapshot_download\n",
    "from html import escape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device: str = \"mps\" if torch.has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "_ = torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Model loading should go in the library, tokenizer wrapping is cursed\n",
    "\n",
    "MODEL = \"llama-13b\"\n",
    "if 'llama' in MODEL:\n",
    "    model_path: str = \"../models/llama-13B\" if MODEL == 'llama-13b' else snapshot_download(\"decapoda-research/llama-7b-hf\")\n",
    "    config = LlamaConfig.from_pretrained(model_path)\n",
    "    # decapoda-research llama is kinda fucked\n",
    "    config.update({\"bos_token_id\": 1, \"eos_token_id\": 2, \"pad_token_id\": 0})\n",
    "\n",
    "    with init_empty_weights():\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "        model.tie_weights() # in case checkpoint doesn't contain duplicate keys for tied weights\n",
    "\n",
    "    model = load_checkpoint_and_dispatch(model, model_path, device_map={'': device}, dtype=torch.float16, no_split_module_classes=[\"LlamaDecoderLayer\"])\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "    # Fancy unicode underscore doesn't overlap with normal underscore!\n",
    "    model.to_str_tokens = lambda t: [t.replace('▁', ' ') for t in tokenizer.tokenize(t)]\n",
    "else:\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model.to_str_tokens = lambda t: [t.replace('Ġ', ' ') for t in tokenizer.tokenize(t)]\n",
    "\n",
    "model.tokenizer = tokenizer\n",
    "# In steering experimentation spaces were found to work well, this makes no sense and I hate it.\n",
    "tokenizer.pad_token_id = int(model.tokenizer.encode(\" \")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling kwargs for gpt2-xl, llama ideal may be different!\n",
    "sampling_kwargs: Dict[str, Union[float, int]] = {\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.3,\n",
    "    \"freq_penalty\": 1.0,\n",
    "    \"num_comparisons\": 3,\n",
    "    \"tokens_to_generate\": 50,\n",
    "    \"seed\": 0,  # For reproducibility\n",
    "}\n",
    "get_x_vector_preset: Callable = partial(\n",
    "    get_x_vector,\n",
    "    pad_method=\"tokens_right\",\n",
    "    model=model,\n",
    "    custom_pad_id=tokenizer.pad_token_id,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore AVE vectors with a perplexity dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move some of this to the library\n",
    "\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_diff_vector(prompt_add: str, prompt_sub: str, layer: int):\n",
    "    return aa.get_diff_vector(model, tokenizer, prompt_add, prompt_sub, layer)\n",
    "\n",
    "\n",
    "@lru_cache\n",
    "def run_aa(coeff: float, layer: int, prompt_add: str, prompt_sub: str, texts: tuple[str], loss_ignore_mod_tokens: bool = False):\n",
    "    # todo: could compute act_diff for all layers at once, then a single fwd pass of cost for changing layer.\n",
    "    act_diff = coeff * get_diff_vector(prompt_add, prompt_sub, layer)\n",
    "    blocks = aa.get_blocks(model)\n",
    "    with aa.pre_hooks([(blocks[layer], aa.get_hook_fn(act_diff))]):\n",
    "        inputs = tokenizer(list(texts), return_tensors='pt', padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        output = model(**inputs)\n",
    "\n",
    "    logprobs = torch.log_softmax(output.logits.to(torch.float32), -1)\n",
    "    token_loss = -logprobs[..., :-1, :].gather(dim=-1, index=inputs['input_ids'][..., 1:, None])[..., 0]\n",
    "    if loss_ignore_mod_tokens:\n",
    "        loss = token_loss[..., act_diff.shape[1]:].mean(1) # skip the screwed-up modified tokens\n",
    "    else:\n",
    "        # TODO: Make generic over injection location (along with everything else...)\n",
    "        loss = token_loss.mean(1)\n",
    "\n",
    "    return loss, token_loss, logprobs\n",
    "\n",
    "\n",
    "def run_aa_interactive_diff(*args, topk=False, **kwargs):\n",
    "    assert len(kwargs['texts'][0]) == len(kwargs['texts'][1]), 'must have same number of positive/negative examples'\n",
    "    split_at = len(kwargs['texts'][0])\n",
    "    kwargs['texts'] = kwargs['texts'][0] + kwargs['texts'][1]\n",
    "\n",
    "    loss, token_loss, mod_logprobs = run_aa(*args, **kwargs)\n",
    "    abs_loss, abs_token_loss, abs_logprobs = run_aa(0., 0, '', '', texts=kwargs['texts']) # cached\n",
    "    diff, diff_token_loss = (loss - abs_loss), (token_loss - abs_token_loss)\n",
    "\n",
    "    print(f'loss change: {[round(l, 4) for l in diff.tolist()]}')\n",
    "    print(f'{(diff.argsort()[:split_at] < split_at).sum()} / {len(kwargs[\"texts\"])} most likely texts are good')\n",
    "\n",
    "    # If you have the convention that texts[0] is \"similar\" to texts[1] (e.g. \"I love you\" v.s. \"I hate you\") then\n",
    "    # a loss based on pairwise distances is interpretable.\n",
    "    # If you don't have that convention, this loss still works, just rearrange.\n",
    "    sloss = (diff[:split_at] - diff[split_at:]).mean()\n",
    "    print(f'separation loss: {sloss:.4f}')\n",
    "    print(f'change in loss: {diff.mean():.4f}')\n",
    "\n",
    "    # Negative loss gives logprobs\n",
    "    display(HTML(show_colors(\n",
    "        kwargs['texts'], abs_logprobs, mod_logprobs,\n",
    "        -diff_token_loss, topk=topk,\n",
    "        steering_prompts=(kwargs['prompt_add'], kwargs['prompt_sub']))\n",
    "    ))\n",
    "\n",
    "\n",
    "def show_colors(\n",
    "        texts: list[str],\n",
    "        logprobs_nom,\n",
    "        logprobs_mod,\n",
    "        token_logprobs_diff,\n",
    "        topk=False,\n",
    "        steering_prompts: Tuple[str, str] = None,\n",
    "):\n",
    "    # compute topk for unmodified and modified model\n",
    "    assert len(texts) == len(token_logprobs_diff)\n",
    "    assert steering_prompts is None or len(steering_prompts) == 2 # add and sub vectors\n",
    "\n",
    "    # tokenize steering prompts\n",
    "    if steering_prompts:\n",
    "        steering_ids = tokenizer(list(steering_prompts), padding=True)['input_ids']\n",
    "        steering_toks = [tokenizer.batch_decode(ids) for ids in steering_ids]\n",
    "        assert len(steering_toks[0]) == len(steering_toks[1]), 'Padding steering_toks failed'\n",
    "        steering_toks = [[escape(tok.replace(' ', \"' '\")) for tok in prompt] for prompt in steering_toks]\n",
    "\n",
    "    show_topk = topk # topk is shadowed later\n",
    "    if show_topk:\n",
    "        topk_nom, topk_mod = torch.topk(logprobs_nom, 5), torch.topk(logprobs_mod, 5)\n",
    "        seq_len = logprobs_nom.shape[1]\n",
    "\n",
    "    html = ''\n",
    "    for i, (text, logprobs_diff) in enumerate(sorted(zip(texts, token_logprobs_diff), key=lambda x: -x[1].mean().abs().item())):\n",
    "        # TODO: Specialize inside loop to another function\n",
    "\n",
    "        if show_topk:\n",
    "            topk_htmls = []\n",
    "            for topk in [topk_nom, topk_mod]:\n",
    "                # predictions shift forward one token\n",
    "                # TODO: optimize. the tokenization line is responsible for ~90% of the time in show_colors (~0.5s)\n",
    "                topk_tokens = [tokenizer.batch_decode(topk.indices[i, j]) for j in range(seq_len)]\n",
    "                topk_htmls.append([colored_tokens(topk_tokens[pos], topk.values[i][pos].exp().tolist(), inject_css=False, low=0, high=1) for pos in range(seq_len)])\n",
    "\n",
    "\n",
    "        str_tokens = model.to_str_tokens(text)\n",
    "        logprobs_diff = logprobs_diff[:len(str_tokens)]\n",
    "        colored_html = colored_tokens(\n",
    "            str_tokens,\n",
    "            logprobs_diff.tolist(),\n",
    "            [\n",
    "                f'ΔLogp: {l:.2f}<br>'\n",
    "                + (f'Steering: {steering_toks[0][t]} - {steering_toks[1][t]}<br>' if steering_prompts and t < len(steering_toks[0]) else '') # TODO: Make generic over injection location (don't assume start)\n",
    "                + (f'TopkNom: {topk_htmls[0][t]}<br>TopkMod: {topk_htmls[1][t]}' if show_topk else '')\n",
    "                for t, l in enumerate(logprobs_diff)\n",
    "            ],\n",
    "            inject_css=(i==len(texts)-1),\n",
    "        )\n",
    "        html += f'<p>ΔLoss: <b>{-logprobs_diff.mean():.2f}</b> - ' + colored_html + '</p>'\n",
    "\n",
    "    return html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Love v.s. Hate\n",
    "\n",
    "Using the above tools to investigate the Love/Hate vector. Feel free to copy these cells to investigate multiple vectors at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3392e6606db4479cacf9870b12bda59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='Show topk (bit slow)'), FloatSlider(value=1.0, descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.run_aa_interactive_diff(*args, topk=False, **kwargs)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import Checkbox\n",
    "\n",
    "# using a tuple here allows hashing for cache lookup\n",
    "texts = (\n",
    "    # We want to increase the probability of these\n",
    "    ('I hate you because I love you\\nI love you', \"I hate you because you're so beautiful.\", 'I hate you because I love you\\nThe world is a stage'),\n",
    "\n",
    "    # ...And decrease these\n",
    "    ('I hate you because you are a girl.', \"I hate you because you're not me.\\nI hate you because I am me.\", 'I hate you because you are a man and I am a woman.')\n",
    ")\n",
    "\n",
    "widgets = dict(\n",
    "    coeff=FloatSlider(value=1, min=0, max=10),\n",
    "    layer=IntSlider(value=0, min=0, max=39),\n",
    "    prompt_add=Text('Love'), prompt_sub=Text('Hate'),\n",
    "    texts=fixed(texts),\n",
    "    topk=Checkbox(value=False, description='Show topk (bit slow)'),\n",
    "    loss_ignore_mod_tokens=Checkbox(value=False, description='Include modified tokens in loss')\n",
    ")\n",
    "interact(run_aa_interactive_diff, **widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you find a good vector, attempt generation\n",
    "PROMPT = \"I hate you because\"\n",
    "\n",
    "summand: List[ActivationAddition] = [\n",
    "    *get_x_vector_preset(\n",
    "        prompt1=widgets['prompt_add'].value,\n",
    "        prompt2=widgets['prompt_sub'].value,\n",
    "        coeff=widgets['coeff'].value,\n",
    "        act_name=widgets['layer'].value,\n",
    "    )\n",
    "]\n",
    "\n",
    "kwargs = sampling_kwargs.copy()\n",
    "prompt_batch = [PROMPT] * kwargs.pop('num_comparisons')\n",
    "results = get_n_comparisons(\n",
    "    model=model,\n",
    "    prompts=prompt_batch,\n",
    "    additions=summand,\n",
    "    **kwargs,\n",
    ")\n",
    "pretty_print_completions(results=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I hate you because you are a girl.\\nYou are a girl and I am not.\\nI am not a girl and you are.\\nYou have to be the first one to say it, then we will see who is right.',\n",
       " \"I hate you because you're so beautiful and i'm so ugly\\ni hate you because i love you and i love you because i hate you\\ni wish that we could be together but then again what would people think?\\nwe can't be together\",\n",
       " \"I hate you because you're so beautiful.\\nI love you, and, in a way, I hate you.\\nI love your smile and your eyes.\\nBut I hate the fact that we can't be together.\\nI love how sweet\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can copy results to use for loss comparison\n",
    "df = results[results.is_modified == True]\n",
    "tuple(p+c for p,c in zip(df.prompts, df.completions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ave",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
