# Besprechungsnotizen

- ich benutze jetzt Python 3.10
- alle cluster sentences die "not" enthalten heraus filtern in cluster preselect?
	- oder schon bei der Generierung die entsprechenden Sätze nicht speichern?
	- ich muss es auf jeden Fall berücksichtigen, das sentence embedding kann das not nicht zuverlässig berücksichtigen
	- das muss ich in der Thesis beschreiben, aber brauche ich dafür auch Daten oder reicht es wenn ich sage "Ich habe herausgefunden dass es besser ist not herauszufiltern"
	- *We reject style attributes that are > 120 characters, are not pure ASCII, contain “not” or “avoids” (negative statements), or contain quotes or the word “mentions” (these attributes tend to be more relevant to content than style).* ([[@patelLearningInterpretableStyle2023]])
- [[@patelLearningInterpretableStyle2023]] benutzen die targeted prompts für die ersten 87 style attributes. Ich würde spontan sagen, dass es besser ist auf die cluster selection zu vertrauen, einige von den targeted prompts wirken nicht unbedingt hilfreich (z.b. self harm).
	- wobei es sein könnte, dass sie im Moment rausgeschmissen werden, weil sie für alle Gruppen verwendet werden
---
- Es gibt nicht wirklich aktuelle encoder Modelle, am besten geeignet scheint tatsächlich DeBerta zu sein
	- einen decoder oder encoder-decoder zu verwenden erscheint nicht sinnvoll
- style embedding
	- Patel et al. benutzen triplet loss um ein embedding zu erzeugen, in dem Texte von unterschiedlichen Autoren verglichen werden -> distance zwischen embeddings für authorship attribution
	- in meinem Fall mit Gruppen?
		- deutlich weniger Gruppen, aber deutlich mehr Beispiele pro Gruppe als pro Author
---
- bisheriges Training anschauen
	- debugging wegen slurm
	- "text" kann nicht in preprocessed dataset bleiben, ansonsten wirft pytorch einen Fehler
	- out of memory error trotz 40GB GPU RAM; deberta ist eigentlich nicht so groß (Problem tritt auch mit deberta-v3-small auf, das hat nur 44M Parameter)