# Besprechungsnotizen
## Thesis title
### bisherige Ideen (siehe [[Questions]])
- (Producing interpretable style vectors and steering large language models towards user appropriate text generation)
- Steering large language models towards user appropriate text generation with interpretable style vectors
- Steering large language models towards group-specific explanation generation with interpretable style vectors
### neue Ideen
- Producing interpretable style vectors and steering large language models towards group-specific explanation generation
- Producing interpretable style vectors and adopting them in steering large language models towards group-specific explanation generation
- Developing Interpretable Style Vectors to steer Large Language Models in Group-Specific Explanation Generation -> in/for/towards?
- Steering Large Language Models with Interpretable Style Vectors for Group-Specific Explanation Generation


## Soft prompting vs. activation addition
- Soft prompting ([[@lesterPowerScaleParameterEfficient2021]]) lernt durch backpropagation, activation addition ([[@turnerActivationAdditionSteering2024]]) Vektoren werden durch forward propagation erzeugt
	- dadurch wird beim soft prompting der gleiche embedding space verwendet wie das Model ihn selbst benutzt
	- allerdings muss der soft prompt trainiert werden, was aufwendiger ist
- ActAdd baut auf der Annahme auf, dass high-level concepts linear im internen representation state des Models abgebildet werden
	- [The Linear Representation Hypothesis and the Geometry of Large Language Models](https://arxiv.org/abs/2311.03658)
	- [Understanding intermediate layers using linear classifier probes](https://arxiv.org/abs/1610.01644)
	- dadurch könnte es gut möglich sein, ActAdd Vektoren für einzelne Style Attribute zu finden und diese dann zu kombinieren; das würde das steering mit einer großen Anzahl an style attributes erst sinnvoll möglich machen
	- soft prompts müssten hingegen wahrscheinlich für jede Kombination aus style Vektoren die verwendet wird trainiert werden; für das Training würden dann immer mehrere Beispiele benötigt werden
- Der Code für [Contrastive Activation Addition](https://aclanthology.org/2024.acl-long.828.pdf) zeigt, dass die Vektoren mit wenigen Beispielen erstellt wurden; diese waren nach dem Muster dass in jedem Prompt zuerst ein Setting beschrieben wurde und dann eine Frage gestellt wurde mit zwei Antwortmöglichkeiten A und B. Pro prompt gab es dann auch ein steering Attribut für das der ActAdd Vektor bestimmt wurde und eine Angabe welches die positive und negative Antwort ist. Die ActAdd Vektoren wurden für alle Layer gespeichert.
	- in meinem Fall wäre es optimal wenn es möglich wäre alles unsupervised zu machen, da ja auch die style attribute unsupervised gefunden werden

### Mögliche Umsetzung von activation addition
- Beispielsätze, die style Attributen (nicht) entsprechen, können erzeugt werden indem die style Attribute zum Prompt hinzugefügt werden
	- alternative könnten auch Beispielsätze aus dem Datensatz verwendet werden, die eine hohe bzw. niedrige Similarity zu dem Attribut haben
- Es wird ein Prompt erzeugt nach dem Schema "Select the better sentence. (A) <sentence_1> (B) <sentence_2>" wobei sentence_1 und sentence_2 abwechselnd die positive und negative Sätze sind.
	- eventuell kann das style Attribut auch mit in den Prompt geschrieben werden
	- der positive und negative Prompt unterscheiden sich, indem am Ende die positive oder negative Antwort (also "(A)" oder "(B)") angefügt wird; allerdings nach dem <|assisstant|> token
	- anschließend werden vom Model die logits für die Prediction des nächsten Tokens produziert und die Activation Vektoren aus allen Layern ausgelesen und gespeichert

## Sonstiges
- Zu ActAdd gibt es noch ein Paper von den gleichen Autoren was zu großen Teilen identisch zu sein scheint? https://arxiv.org/pdf/2308.10248
- Die Methode wurde auch in einem anderen Paper von z.T. anderen Autoren vorgestellt (Turner ist auch Co-Autor) https://aclanthology.org/2024.acl-long.828/

## Anmerkungen
- Activation Vector Extraction wie Konen et al. machen