---
category: literaturenote
tags: [style_transfer]
citekey: wegmannSameAuthorJust2022
---
# Same author or just same topic? Towards content-independent style representations

> [!info]-
> **FirstAuthor**:: Wegmann, Anna  
> **Author**:: Schraagen, Marijn  
> **Author**:: Nguyen, Dong  
> ---> **FirstEditor**:: Gella, Spandana  
> **Editor**:: He, He  
> **Editor**:: Majumder, Bodhisattwa Prasad  
> **Editor**:: Can, Burcu  
> **Editor**:: Giunchiglia, Eleonora  
> **Editor**:: Cahyawijaya, Samuel  
> **Editor**:: Min, Sewon  
> **Editor**:: Mozes, Maximilian  
> **Editor**:: Li, Xiang Lorraine  
> **Editor**:: Augenstein, Isabelle  
> **Editor**:: Rogers, Anna  
> **Editor**:: Cho, Kyunghyun  
> **Editor**:: Grefenstette, Edward  
> **Editor**:: Rimell, Laura  
> **Editor**:: Dyer, Chris  
> ---    
> **Title**:: Same author or just same topic? Towards content-independent style representations  
> **Year**:: 2022   
> **Citekey**:: wegmannSameAuthorJust2022  
> **Type**:: conferencePaper  
> **Publisher**:: Association for Computational Linguistics  
> **Location**:: Dublin, Ireland   
> **Pages**:: 249–268  
> **DOI**:: 10.18653/v1/2022.repl4nlp-1.26
> ---
> Linguistic style is an integral component of language. Recent advances in the development of style representations have increasingly used training objectives from authorship verification (AV)”:” Do two texts have the same author? The assumption underlying the AV training task (same author approximates same writing style) enables self-supervised and, thus, extensive training. However, a good performance on the AV task does not ensure good “general-purpose” style representations. For example, as the same author might typically write about certain topics, representations trained on AV might also encode content information instead of style alone. We introduce a variation of the AV training task that controls for content using conversation or domain labels. We evaluate whether known style dimensions are represented and preferred over content information through an original variation to the recently proposed S℡ framework. We find that representations trained by controlling for conversation are better than representations trained with domain or no content control at representing style independent from content.

## Notes
%% begin notes %%

%% end notes %%

## Annotations



%% Import Date: 2024-11-25T13:54:58.076+01:00 %%
