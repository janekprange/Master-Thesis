---
category: literaturenote
tags: [Computer_Science/Computation_and_Language, Computer_Science/Computers_and_Society]
citekey: gilardiChatGPTOutperformsCrowdworkers2023
---
# ChatGPT outperforms crowd-workers for text-annotation tasks

> [!info]-
> **FirstAuthor**:: Gilardi, Fabrizio  
> **Author**:: Alizadeh, Meysam  
> **Author**:: Kubli, MaÃ«l  
> ---    
> **Title**:: ChatGPT outperforms crowd-workers for text-annotation tasks  
> **Year**:: 2023   
> **Citekey**:: gilardiChatGPTOutperformsCrowdworkers2023  
> **Type**:: journalArticle  
> **Journal**:: *Proceedings of the National Academy of Sciences*  
> **Volume**:: 120  
> **Issue**:: 30   
> **Pages**:: e2305016120  
> **DOI**:: 10.1073/pnas.2305016120
> ---
> Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.

## Notes
%% begin notes %%

%% end notes %%

## Annotations



%% Import Date: 2024-10-01T22:57:28.158+02:00 %%
