---
category: literaturenote
tags: [Computer_Science/Computation_and_Language]
citekey: patelLearningInterpretableStyle2023
---
> [!info]-
> **FirstAuthor**:: Patel, Ajay  
> **Author**:: Rao, Delip  
> **Author**:: Kothary, Ansh  
> **Author**:: McKeown, Kathleen  
> **Author**:: Callison-Burch, Chris  
> ---    
> **Title**:: Learning Interpretable Style Embeddings via Prompting LLMs  
> **Year**:: 2023   
> **Citekey**:: patelLearningInterpretableStyle2023  
> **Type**:: preprint  
> **DOI**:: 10.48550/arXiv.2305.12696
> ---
> Style representation learning builds content-independent representations of author style in text. Stylometry, the analysis of style in text, is often performed by expert forensic linguists and no large dataset of stylometric annotations exists for training. Current style representation learning uses neural methods to disentangle style from content to create style vectors, however, these approaches result in uninterpretable representations, complicating their usage in downstream applications like authorship attribution where auditing and explainability is critical. In this work, we use prompting to perform stylometry on a large number of texts to create a synthetic dataset and train human-interpretable style representations we call LISA embeddings. We release our synthetic stylometry dataset and our interpretable style models as resources.

## Notes
%% begin notes %%

%% end notes %%

## Annotations



%% Import Date: 2024-09-27T10:43:57.640+02:00 %%
